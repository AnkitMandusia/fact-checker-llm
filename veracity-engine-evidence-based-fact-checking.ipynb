{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q \\\n    gradio \\\n    google-generativeai \\\n    SpeechRecognition \\\n    pydub \\\n    gTTS \\\n    langchain \\\n    langchain-google-genai \\\n    langchain_community \\\n    langchain-huggingface \\\n    faiss-cpu \\\n    ragas\\\n    langchain_tavily\\\n    beautifulsoup4 \\\n    selenium\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:37:37.408881Z","iopub.execute_input":"2025-09-17T16:37:37.409188Z","iopub.status.idle":"2025-09-17T16:38:15.747187Z","shell.execute_reply.started":"2025-09-17T16:37:37.409155Z","shell.execute_reply":"2025-09-17T16:38:15.745531Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.1/279.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m947.6/947.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gradio as gr\nimport speech_recognition as sr\nimport os\nimport uuid\nimport logging\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport collections\nimport concurrent.futures\nimport tempfile\nimport shutil\nfrom gtts import gTTS\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate\nfrom langchain_core.runnables import RunnableSequence\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_tavily import TavilySearch\nfrom kaggle_secrets import UserSecretsClient\nimport nltk\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Download NLTK data\nnltk.download('punkt')\n\n# --- CONFIGURATION ---\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# Retrieve API keys from Kaggle Secrets\ntry:\n    user_secrets = UserSecretsClient()\n    API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n    TAVILY_API_KEY = user_secrets.get_secret(\"TAVILY_API_KEY\")\n    logging.info(\"Successfully retrieved API keys from Kaggle Secrets.\")\nexcept Exception as e:\n    API_KEY = None\n    TAVILY_API_KEY = None\n    logging.warning(f\"Could not retrieve API keys from Kaggle Secrets: {e}. The application may not function.\")\nif TAVILY_API_KEY:\n    os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n\n# --- INITIALIZE SERVICES ---\nrecognizer = sr.Recognizer()\ntry:\n    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=API_KEY, temperature=0.8)  # Increased temperature\nexcept Exception as e:\n    logging.error(f\"Failed to initialize Gemini LLM: {e}\")\n    llm = None\nvector_db = None\nembedding_function = None\ntry:\n    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    logging.info(\"✅ HuggingFace embeddings initialized.\")\nexcept Exception as e:\n    logging.error(f\"Failed to initialize embeddings: {e}\")\n\n# ================================\n# SECTION 1: EVALUATION & LOGGING\n# ================================\neval_logger = logging.getLogger(\"evaluation_logger\")\neval_logger.setLevel(logging.INFO)\nfile_handler = logging.FileHandler(\"fact_checks.log\")\nformatter = logging.Formatter('%(asctime)s - %(message)s')\nfile_handler.setFormatter(formatter)\neval_logger.addHandler(file_handler)\n\ndef log_fact_check(claim: str, evidence: str, llm_response: str):\n    log_entry = {\"claim\": claim, \"evidence_context\": evidence, \"llm_response\": llm_response}\n    eval_logger.info(json.dumps(log_entry))\n    logging.info(\"Fact-check entry logged for evaluation.\")\n\ndef compute_keyword_overlap(explanation: str, evidence: str) -> float:\n    \"\"\"Compute a fallback score based on keyword overlap.\"\"\"\n    explanation_tokens = set(word_tokenize(explanation.lower()))\n    evidence_tokens = set(word_tokenize(evidence.lower()))\n    common_tokens = explanation_tokens.intersection(evidence_tokens)\n    if not explanation_tokens or not evidence_tokens:\n        return 0.0\n    overlap = len(common_tokens) / len(explanation_tokens)\n    return round(overlap, 4)\n# Initialize Selenium WebDriver\ndef init_selenium_driver():\n    try:\n        chrome_options = Options()\n        chrome_options.add_argument(\"--headless\")\n        chrome_options.add_argument(\"--disable-gpu\")\n        chrome_options.add_argument(\"--no-sandbox\")\n        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n        driver = webdriver.Chrome(options=chrome_options)\n        return driver\n    except Exception as e:\n        logging.warning(f\"Failed to initialize Selenium driver: {e}\")\n        return None\n\n\ndef simple_llm_check(claim: str, explanation: str, evidence: str) -> float:\n    \"\"\"Simple LLM-based check for fallback scoring.\"\"\"\n    if not llm:\n        return 0.0\n    prompt = PromptTemplate(\n        template=\"\"\"\n        You are a fact-checker evaluating whether an explanation accurately reflects the provided evidence for a given claim.\n        - Claim: {claim}\n        - Explanation: {explanation}\n        - Evidence: {evidence}\n        Instructions:\n        1. Compare the explanation to the evidence.\n        2. Assess if the explanation's key points (e.g., facts, conclusions) are supported by the evidence.\n        3. Return a score from 0 to 1, where 1 is completely accurate and 0 is completely inaccurate.\n        Output: Score: [0-1]\n        \"\"\",\n        input_variables=[\"claim\", \"explanation\", \"evidence\"]\n    )\n    chain = prompt | llm | StrOutputParser()\n    try:\n        result = chain.invoke({\"claim\": claim, \"explanation\": explanation, \"evidence\": evidence})\n        score = float(result.split(\"Score: \")[-1].strip())\n        return round(score, 4)\n    except Exception as e:\n        logging.warning(f\"Simple LLM check failed: {e}\")\n        return 0.0\ndef evaluate_fact_check(claim, explanation, evidence, responses, sources, llm=None):\n    def length_score(text, min_words=80, max_words=400):\n        words = text.split()\n        if len(words) < min_words:\n            return len(words) / min_words\n        if len(words) > max_words:\n            return max_words / len(words)\n        return 1.0\n\n    def neutrality_score(text):\n        biased_words = [\"definitely\", \"obviously\", \"clearly\", \"undoubtedly\"]\n        penalty = sum(text.lower().count(w) for w in biased_words)\n        return max(0.0, 1 - (penalty * 0.1))\n\n    def source_coverage(evidence, sources):\n        return sum(1 for s in sources if s in evidence) / max(1, len(sources))\n\n    def verdict_agreement(responses):\n        verdicts = [r[\"verdict\"] for r in responses if \"verdict\" in r]\n        if not verdicts:\n            return 0.0\n        majority = max(set(verdicts), key=verdicts.count)\n        return verdicts.count(majority) / len(verdicts)\n\n    def keyword_relevance(claim, explanation):\n        from nltk.tokenize import word_tokenize\n        claim_words = set(word_tokenize(claim.lower()))\n        expl_words = set(word_tokenize(explanation.lower()))\n        overlap = claim_words & expl_words\n        return len(overlap) / max(1, len(claim_words))\n\n    def llm_grounding_score(claim, explanation, evidence):\n        if not llm:\n            return 0.5\n        try:\n            return simple_llm_check(claim, explanation, evidence)\n        except:\n            return 0.5\n\n    return {\n        \"content_quality\": round((length_score(explanation) + neutrality_score(explanation)) / 2, 2),\n        \"evidence_grounding\": round((source_coverage(evidence, sources) + llm_grounding_score(claim, explanation, evidence)) / 2, 2),\n        \"consistency\": round(verdict_agreement(responses), 2),\n        \"relevance\": round(keyword_relevance(claim, explanation), 2),\n    }\n\n\n# ================================\n# SECTION 2: CORE PIPELINE\n# ================================\ndef tavily_search(query: str, num_results: int = 15):\n    if not TAVILY_API_KEY:\n        logging.error(\"Tavily API key not available.\")\n        return []\n    try:\n        search_tool = TavilySearch(max_results=num_results)\n        response_dict = search_tool.invoke(query)\n        if isinstance(response_dict, dict) and \"results\" in response_dict:\n            return [res.get(\"url\") for res in response_dict[\"results\"] if res.get(\"url\")]\n        if isinstance(response_dict, list) and all(isinstance(res, dict) for res in response_dict):\n            return [res.get(\"url\") for res in response_dict if res.get(\"url\")]\n        logging.error(f\"Tavily search returned an unexpected format: {response_dict}\")\n        return []\n    except Exception as e:\n        logging.error(f\"Tavily search failed: {e}\")\n        return []\n\ndef get_content_from_urls(urls: list[str]) -> str:\n    all_content = []\n    driver = init_selenium_driver()\n    for url in urls:\n        try:\n            if driver:\n                driver.get(url)\n                WebDriverWait(driver, 15).until(\n                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n                )\n                # Scroll multiple times to load full content\n                for _ in range(3):\n                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n                    WebDriverWait(driver, 5).until(\n                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n                    )\n                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n            else:\n                headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n                response = requests.get(url, headers=headers, timeout=15)\n                response.raise_for_status()\n                soup = BeautifulSoup(response.text, \"html.parser\")\n            \n            for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\", \"form\", \"iframe\"]):\n                element.decompose()\n            \n            content_selectors = [\n                (\"article\", None),\n                (\"main\", None),\n                (\"div\", {\"class\": [\"content\", \"article-body\", \"post-content\", \"entry-content\", \"article-content\"]}),\n                (\"section\", {\"class\": [\"content\", \"article\"]}),\n                (\"div\", {\"id\": [\"content\", \"main-content\", \"article\"]}),\n                (\"div\", {\"role\": \"main\"}),\n            ]\n            text = None\n            for tag, attrs in content_selectors:\n                main_content = soup.find(tag, attrs=attrs)\n                if main_content:\n                    text = main_content.get_text(separator=\" \", strip=True)\n                    break\n            if not text:\n                text = soup.get_text(separator=\" \", strip=True)\n            \n            if len(text.strip()) < 300:\n                logging.warning(f\"Content from {url} is too short, likely navigational.\")\n                continue\n            all_content.append(f\"Source: {url}\\nContent: {text[:8000]}...\")\n        except Exception as e:\n            logging.warning(f\"Could not fetch {url}: {e}\")\n    if driver:\n        driver.quit()\n    return \"\\n\\n---\\n\\n\".join(all_content)\n\ndef vet_sources(urls: list[str]) -> list[str]:\n    reputable_domains = [\n        \"reuters.com\", \"apnews.com\", \"bbc.com\", \"nytimes.com\", \"wsj.com\",\n        \"washingtonpost.com\", \"theguardian.com\", \"snopes.com\", \"politifact.com\",\n        \"healthline.com\", \"medlineplus.gov\", \"harvard.edu\"\n    ]\n    vetted = [u for u in urls if any(d in u for d in reputable_domains)]\n    return vetted if vetted else urls\n\ndef synthesize_evidence(claim: str, evidence: str, consistency_level: int = 3) -> list:\n    if not llm:\n        return [{\"verdict\": \"Error\", \"explanation\": \"LLM not available.\"}]\n    prompt_template = \"\"\"\n    **Role:** You are a professional, unbiased fact-checker.\n    **Context:** You have been given a user's claim and the text content extracted from several online sources.\n    **Instructions (Chain-of-Thought):**\n    1. **Analyze Evidence:** Review the content to identify consensus and contradictions.\n    2. **Formulate Verdict:** Choose the most appropriate verdict: [True, Mostly True, Half True, Misleading, False, Unverifiable].\n    3. **Generate Explanation:** Write a detailed, neutral explanation for your verdict.\n    4. **Format Output:** Present your findings in the strict format:\n        VERDICT: [Your chosen verdict]\n        EXPLANATION: [Your detailed explanation]\n    **User's Claim:** \"{claim}\"\n    **Evidence from Sources:**\n    {evidence}\n    \"\"\"\n    prompt = PromptTemplate(template=prompt_template, input_variables=[\"claim\", \"evidence\"])\n    chain = prompt | llm | StrOutputParser()\n    def run_once(_):\n        try:\n            resp = chain.invoke({\"claim\": claim, \"evidence\": evidence})\n            verdict = resp.split(\"VERDICT:\")[1].split(\"EXPLANATION:\")[0].strip()\n            explanation = resp.split(\"EXPLANATION:\")[1].strip()\n            return {\"verdict\": verdict, \"explanation\": explanation}\n        except Exception as e:\n            logging.error(f\"Error during synthesis attempt: {e}\")\n            return {\"verdict\": \"Error\", \"explanation\": str(e)}\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        responses = list(executor.map(run_once, range(consistency_level)))\n    return responses\n\ndef get_consistent_response(responses: list) -> tuple[str, str]:\n    if not responses:\n        return \"Error\", \"No responses generated.\"\n    counts = collections.Counter(r[\"verdict\"] for r in responses)\n    majority = counts.most_common(1)[0][0]\n    for r in responses:\n        if r[\"verdict\"] == majority:\n            logging.info(f\"Majority verdict '{majority}' chosen: {counts}\")\n            return majority, r[\"explanation\"]\n    return responses[0][\"verdict\"], responses[0][\"explanation\"]\n\n# ================================\n# SECTION 3: INPUT/OUTPUT HANDLERS\n# ================================\ndef speech_to_text(audio_file):\n    if not audio_file: return \"\"\n    try:\n        tmp_wav = tempfile.mktemp(suffix=\".wav\")\n        audio = AudioSegment.from_file(audio_file)\n        audio.export(tmp_wav, format=\"wav\")\n        with sr.AudioFile(tmp_wav) as source:\n            audio_data = recognizer.record(source)\n        os.remove(tmp_wav)\n        return recognizer.recognize_google(audio_data)\n    except Exception as e:\n        logging.error(f\"Speech-to-text error: {e}\")\n        return f\"Error: {e}\"\n\ndef text_to_speech(text):\n    \"\"\"Generates speech using Google TTS and returns a file path.\"\"\"\n    try:\n        tts = gTTS(text=text, lang='en')\n        file_path = f\"response_{uuid.uuid4().hex}.mp3\"\n        tts.save(file_path)\n        return file_path\n    except Exception as e:\n        logging.error(f\"TTS error: {e}\")\n        return None\n\n# ================================\n# SECTION 4: MAIN PIPELINE\n# ================================\ndef fact_checking_pipeline(audio_input, text_input):\n    global vector_db\n    claim = speech_to_text(audio_input) if audio_input else text_input.strip()\n    if not claim:\n        return \"Please provide a claim.\", \"Unverifiable\", None, \"No sources analyzed.\"\n    urls = tavily_search(claim)\n    if not urls:\n        return \"No online sources found.\", \"Unverifiable\", None, \"No sources.\"\n    vetted = vet_sources(urls)\n    evidence = get_content_from_urls(vetted)\n    if not evidence:\n        return \"Could not fetch content.\", \"Unverifiable\", None, \"No content.\"\n    responses = synthesize_evidence(claim, evidence, consistency_level=3)\n    verdict, explanation = get_consistent_response(responses)\n    log_fact_check(claim, evidence, explanation)\n   \n    audio_resp = text_to_speech(explanation)\n    if embedding_function:\n        metadata = [{\"sources\": \", \".join(vetted)}]\n        if vector_db is None:\n            vector_db = FAISS.from_texts([claim], embedding_function, metadatas=metadata)\n        else:\n            vector_db.add_texts([claim], metadatas=metadata)\n    src_md = \"\\n\".join(f\"- [{u.split('//')[1].split('/')[0]}]({u})\" for u in vetted)\n   # Run evaluation\n    eval_scores = evaluate_fact_check(claim, explanation, evidence, responses, vetted, llm)\n    \n    # Print to logs\n    logging.info(f\"Evaluation Scores: {json.dumps(eval_scores, indent=2)}\")\n    \n    # Return everything including eval\n    return explanation, verdict, audio_resp, src_md, eval_scores\n\n\n# ================================\n# SECTION 5: GRADIO UI\n# ================================\nwith gr.Blocks(theme=gr.themes.Soft(), title=\"Veracity Engine\") as demo:\n    with gr.Tab(\"🚀 Showcase\"):\n        gr.Markdown(\"# 🔎 Veracity Engine: Evidence-Based Fact-Checking\")\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"## Key Features\")\n                gr.Markdown(\n                    \"\"\"\n                    - **🗣️ Voice & Text Input:** Accepts claims via speech or text.\n                    - **🌐 Real-Time Web Search:** Uses the Tavily API for reliable, up-to-date info.\n                    - **🛡️ Source Credibility Analysis:** Prioritizes reputable sources.\n                    - **⚖️ Nuanced Verdicts:** Provides a spectrum of ratings (e.g., True, Misleading).\n                    - **✍️ Evidence Synthesis:** Generates detailed explanations for its verdicts.\n                    - **💾 Evidence Caching:** Uses a FAISS vector database for in-memory caching.\n                    \"\"\"\n                )\n            with gr.Column():\n                gr.Markdown(\"## Skills & Concepts Demonstrated\")\n                gr.Markdown(\n                    \"\"\"\n                    - **Grounded RAG Pipelines:** Combining real-time data with LLM generation.\n                    - **Prompt Engineering:** Using Chain-of-Thought (CoT) and RCI patterns.\n                    - **Self-Consistency:** Running inference multiple times to find the majority verdict.\n                    - **API Integration:** Interfacing with Google Gemini and Tavily Search APIs.\n                    - **Vector Database Caching:** Using FAISS for efficient in-memory storage.\n                    - **LLM-based Evaluation:** Using synthetic QA pairs and LLM-as-a-judge for correctness.\n                    \"\"\"\n                )\n    with gr.Tab(\"Veracity Engine\"):\n        gr.Markdown(\"## Submit a Claim for Verification\")\n        with gr.Row():\n            with gr.Column(scale=1):\n                audio_input = gr.Audio(type=\"filepath\", label=\"Record or Upload Your Claim\")\n                text_input = gr.Textbox(label=\"Or Type Your Claim Here\", placeholder=\"e.g., 'Does drinking coffee help you live longer?'\")\n                submit_button = gr.Button(\"Check Fact\", variant=\"primary\")\n            with gr.Column(scale=2):\n                verdict_output = gr.Label(label=\"Verdict\")\n                explanation_output = gr.Textbox(label=\"Explanation\", lines=8, interactive=False)\n                audio_output = gr.Audio(label=\"Voice Explanation\")\n                sources_output = gr.Markdown(label=\"Evidence Sources\")\n                eval_output = gr.JSON(label=\"Evaluation Scores\")\n\n               \n        submit_button.click(\n        fn=fact_checking_pipeline,\n        inputs=[audio_input, text_input],\n        outputs=[explanation_output, verdict_output, audio_output, sources_output, eval_output],\n        api_name=\"verify_claim\"\n    )\n\n\n# --- LAUNCH ---\nif __name__ == \"__main__\":\n    demo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:47:47.890188Z","iopub.execute_input":"2025-09-17T16:47:47.890558Z","iopub.status.idle":"2025-09-17T16:47:52.095076Z","shell.execute_reply.started":"2025-09-17T16:47:47.890530Z","shell.execute_reply":"2025-09-17T16:47:52.093081Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://032e4e5cef97b360b2.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://032e4e5cef97b360b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}